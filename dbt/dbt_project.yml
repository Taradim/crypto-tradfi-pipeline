name: "data_pipeline"
version: "1.0.0"
config-version: 2
profile: "data_pipeline_portfolio"

# These configurations specify where dbt should look for different types of files.
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"

# Optional: Configure on-run-start hook for additional S3 setup
# Note: S3 credentials are configured via profiles.yml settings
# Uncomment if you need additional S3 configuration:
# on-run-start:
#   - "{{ configure_s3() }}"

# Variables: S3_BUCKET_NAME required for dbt run; default empty for parse/test without env
vars:
  s3_bucket_name: "{{ env_var('S3_BUCKET_NAME', '') }}"

# Configuring models (Plan V1 - Step 3 dbt, Step 4.5 Glue Catalog)
# Full documentation: https://docs.getdbt.com/reference/model-configs
# Bronze/Silver/Gold architecture; schema = Glue database (silver.coingecko, gold.crypto_prices)
# Location = one folder per table (s3://bucket/silver/coingecko/) to avoid schema convergence
models:
  data_pipeline:
    silver:
      description: "Silver layer - read from S3 bronze, write to S3 silver/<table>/"
      +materialized: external
      +glue_register: true
      +glue_database: silver
      # +location: set per model via macro get_external_location(this)
    gold:
      description: "Gold layer - business-ready analytical data"
      +materialized: external
      +glue_register: true
      +glue_database: gold
      # +location: set per model via macro get_external_location(this)
